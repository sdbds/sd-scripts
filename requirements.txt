accelerate==0.33.0
transformers>=4.44.2
diffusers[torch]==0.25.0
ftfy==6.1.1
# albumentations==1.3.0
opencv-python==4.8.1.78
einops==0.8.1
pytorch-lightning==1.9.0
bitsandbytes>=0.45.3
prodigyopt
prodigy-plus-schedule-free
lion-pytorch
scipy
dadaptation
heavyball>=0.24.4
wandb>=0.18.0
pillow-avif-plugin
schedulefree>=1.4.0
torch-optimi==0.2.1
adam-mini>=1.1.1
tensorboard
safetensors==0.4.4
# gradio==3.16.2
altair==4.2.2
easygui==0.98.3
toml==0.10.2
voluptuous==0.13.1
huggingface_hub[hf_xet]>=0.30.2
# for Image utils
imagesize==1.4.1
numpy<=2.0
# for BLIP captioning
# requests==2.28.2
# timm==0.6.12
# fairscale==0.4.13
# for WD14 captioning (tensorflow)
tensorflow
# for WD14 captioning (onnx)
onnx>=1.17.0
# onnxruntime-gpu==1.18.1
# onnxruntime==1.18.1
onnxruntime-gpu==1.20.2 ; sys_platform == 'win32'
onnxruntime-gpu>=1.20.2 ; sys_platform == 'linux'

# for HunYuanDiT
timm==1.0.11
torch==2.7.0
xformers

# this is for onnx: 
protobuf
# open clip for SDXL
# open-clip-torch==2.20.0
# For logging
rich==13.7.0
# for T5XXL tokenizer (SD3/FLUX)
sentencepiece==0.2.0
# for kohya_ss library
-e .
triton-windows>=3.3.0.post19 ; sys_platform == 'win32'
https://github.com/sdbds/flash-attention-for-windows/releases/download/torch270%2Bcu128/flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl ; sys_platform == 'win32'
git+https://github.com/Dao-AILab/flash-attention; sys_platform == 'linux'
optree>=0.13.0
