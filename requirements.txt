accelerate==1.6.0
transformers==4.54.1
diffusers[torch]==0.32.1
ftfy==6.3.1
# albumentations==1.3.0
opencv-python==4.10.0.84
einops==0.7.0
# pytorch-lightning==1.9.0
bitsandbytes
lion-pytorch==0.2.3
schedulefree==1.4
pytorch-optimizer==3.7.0
prodigy-plus-schedule-free==1.9.2
prodigyopt==1.1.2
scipy
dadaptation
heavyball>=0.24.4
wandb>=0.18.0
pillow>=11.3.0
torch-optimi==0.2.1
adam-mini>=1.1.1
tensorboard
safetensors==0.4.5
# gradio==3.16.2
# altair==4.2.2
# easygui==0.98.3
toml==0.10.2
voluptuous==0.15.2
huggingface-hub==0.34.3
# for Image utils
imagesize==1.4.1
numpy
# <=2.0
# for BLIP captioning
# requests==2.28.2
# timm==0.6.12
# fairscale==0.4.13
# for WD14 captioning (tensorflow)
tensorflow
# for WD14 captioning (onnx)
onnx>=1.17.0
# onnxruntime-gpu==1.18.1
# onnxruntime==1.18.1
onnxruntime-gpu==1.20.2 ; sys_platform == 'win32'
onnxruntime-gpu>=1.20.2 ; sys_platform == 'linux'

# for HunYuanDiT
timm==1.0.11
torch==2.7.0
xformers

# this is for onnx: 
protobuf
# open clip for SDXL
# open-clip-torch==2.20.0
# For logging
rich==14.1.0
# for T5XXL tokenizer (SD3/FLUX)
sentencepiece==0.2.1
# for kohya_ss library
-e .
triton-windows>=3.3.0.post19 ; sys_platform == 'win32'
https://github.com/sdbds/flash-attention-for-windows/releases/download/torch270%2Bcu128/flash_attn-2.7.4.post1+cu128torch2.7.0cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl ; sys_platform == 'win32'
git+https://github.com/Dao-AILab/flash-attention; sys_platform == 'linux'
optree>=0.13.0
